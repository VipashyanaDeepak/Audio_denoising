{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7362011,"sourceType":"datasetVersion","datasetId":3959863}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-22T15:23:32.327241Z","iopub.execute_input":"2025-03-22T15:23:32.327549Z","iopub.status.idle":"2025-03-22T15:23:32.629216Z","shell.execute_reply.started":"2025-03-22T15:23:32.327521Z","shell.execute_reply":"2025-03-22T15:23:32.628336Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchaudio\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T15:23:32.630304Z","iopub.execute_input":"2025-03-22T15:23:32.630690Z","iopub.status.idle":"2025-03-22T15:23:37.143308Z","shell.execute_reply.started":"2025-03-22T15:23:32.630660Z","shell.execute_reply":"2025-03-22T15:23:37.142616Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class ValentiniDataset(Dataset):\n    def __init__(self, noisy_files, clean_files, segment_length=48000, hop_length=24000, n_fft=512, hop_stft=128):\n        self.noisy_files = noisy_files\n        self.clean_files = clean_files\n        self.segment_length = segment_length\n        self.hop_length = hop_length\n        self.n_fft = n_fft\n        self.hop_stft = hop_stft\n        self.segments = []\n\n        for idx in range(len(noisy_files)):\n            noisy_audio, sr = torchaudio.load(noisy_files[idx])\n            audio_length = noisy_audio.size(1)\n            num_segments = max(1, (audio_length - segment_length) // hop_length + 1)\n            for seg_idx in range(num_segments):\n                start = seg_idx * hop_length\n                end = start + segment_length\n                self.segments.append((idx, start, end))\n\n    def __len__(self):\n        return len(self.segments)\n\n    def __getitem__(self, index):\n        idx, start, end = self.segments[index]\n        noisy_segment, sr = torchaudio.load(self.noisy_files[idx], frame_offset=start, num_frames=self.segment_length)\n        clean_segment, _ = torchaudio.load(self.clean_files[idx], frame_offset=start, num_frames=self.segment_length)\n        \n        noisy_segment = noisy_segment.squeeze(0)[:self.segment_length]\n        clean_segment = clean_segment.squeeze(0)[:self.segment_length]\n        \n        if noisy_segment.size(0) < self.segment_length:\n            noisy_segment = torch.nn.functional.pad(noisy_segment, (0, self.segment_length - noisy_segment.size(0)))\n        if clean_segment.size(0) < self.segment_length:\n            clean_segment = torch.nn.functional.pad(clean_segment, (0, self.segment_length - clean_segment.size(0)))\n\n        noisy_stft = torch.stft(noisy_segment, n_fft=self.n_fft, hop_length=self.hop_stft, \n                                window=torch.hann_window(self.n_fft), return_complex=True)\n        clean_stft = torch.stft(clean_segment, n_fft=self.n_fft, hop_length=self.hop_stft, \n                                window=torch.hann_window(self.n_fft), return_complex=True)\n\n        noisy_mag = torch.abs(noisy_stft)\n        clean_mag = torch.abs(clean_stft)\n        noisy_logmag = torch.log(1 + noisy_mag).unsqueeze(0)\n        clean_logmag = torch.log(1 + clean_mag).unsqueeze(0)\n\n        noisy_logmag = noisy_logmag[:, :, :123]\n        clean_logmag = clean_logmag[:, :, :123]\n\n        return noisy_logmag, clean_logmag","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T15:23:37.144962Z","iopub.execute_input":"2025-03-22T15:23:37.145342Z","iopub.status.idle":"2025-03-22T15:23:37.153880Z","shell.execute_reply.started":"2025-03-22T15:23:37.145319Z","shell.execute_reply":"2025-03-22T15:23:37.153145Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n\n        self.enc1 = self.conv_block(1, 64)\n        self.pool1 = nn.MaxPool2d(2)\n        self.enc2 = self.conv_block(64, 128)\n        self.pool2 = nn.MaxPool2d(2)\n        self.enc3 = self.conv_block(128, 256)\n        self.pool3 = nn.MaxPool2d(2)\n\n        self.bottleneck = self.conv_block(256, 512)\n\n        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.dec3 = self.conv_block(512, 256)\n        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.dec2 = self.conv_block(256, 128)\n        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2, output_padding=(0, 1))\n        self.dec1 = self.conv_block(128, 64)\n        self.out = nn.Conv2d(64, 1, kernel_size=1)\n\n    def conv_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        e1 = self.enc1(x)      \n        p1 = self.pool1(e1)    \n        e2 = self.enc2(p1)     \n        p2 = self.pool2(e2)    \n        e3 = self.enc3(p2)     \n        p3 = self.pool3(e3)    \n        b = self.bottleneck(p3) \n\n        u3 = self.up3(b)       \n        e3_cropped = e3[:, :, :, :u3.size(3)]  \n        cat3 = torch.cat([u3, e3_cropped], dim=1)  \n        d3 = self.dec3(cat3)   \n        u2 = self.up2(d3)      \n        e2_cropped = e2[:, :, :, :u2.size(3)]  \n        cat2 = torch.cat([u2, e2_cropped], dim=1)  \n        d2 = self.dec2(cat2)   \n        u1 = self.up1(d2)      \n        u1 = nn.functional.pad(u1, (0, 2, 0, 1), mode='constant', value=0)  \n        cat1 = torch.cat([u1, e1], dim=1)  \n        d1 = self.dec1(cat1)   \n        out = self.out(d1)     \n\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T15:23:37.155302Z","iopub.execute_input":"2025-03-22T15:23:37.155581Z","iopub.status.idle":"2025-03-22T15:23:37.172221Z","shell.execute_reply.started":"2025-03-22T15:23:37.155561Z","shell.execute_reply":"2025-03-22T15:23:37.171616Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\ndef train_model(model, train_loader, val_loader, num_epochs, device):\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n\n    best_val_loss = float('inf')\n    \n    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n        model.train()\n        train_loss = 0\n        \n        train_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\", leave=False)\n        for noisy_batch, clean_batch in train_bar:\n            noisy_batch, clean_batch = noisy_batch.to(device), clean_batch.to(device)\n            optimizer.zero_grad()\n            outputs = model(noisy_batch)\n            loss = criterion(outputs, clean_batch)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * noisy_batch.size(0)\n            train_bar.set_postfix({'loss': loss.item()})\n        train_loss /= len(train_loader.dataset)\n\n        model.eval()\n        val_loss = 0\n        \n        val_bar = tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\", leave=False)\n        with torch.no_grad():\n            for noisy_batch, clean_batch in val_bar:\n                noisy_batch, clean_batch = noisy_batch.to(device), clean_batch.to(device)\n                outputs = model(noisy_batch)\n                loss = criterion(outputs, clean_batch)\n                val_loss += loss.item() * noisy_batch.size(0)\n                val_bar.set_postfix({'loss': loss.item()})\n        val_loss /= len(val_loader.dataset)\n\n        scheduler.step(val_loss)\n        \n        tqdm.write(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), '/kaggle/working/best_model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T15:23:37.172838Z","iopub.execute_input":"2025-03-22T15:23:37.173035Z","iopub.status.idle":"2025-03-22T15:23:37.304270Z","shell.execute_reply.started":"2025-03-22T15:23:37.173012Z","shell.execute_reply":"2025-03-22T15:23:37.303552Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def denoise_audio(model, noisy_file, n_fft=512, hop_stft=128, device='cuda'):\n    model.eval()\n    noisy_audio, sr = torchaudio.load(noisy_file)\n    noisy_stft = torch.stft(noisy_audio.squeeze(0), n_fft=n_fft, hop_length=hop_stft, \n                            window=torch.hann_window(n_fft), return_complex=True)\n    noisy_mag = torch.abs(noisy_stft)\n    noisy_phase = torch.angle(noisy_stft)\n    noisy_logmag = torch.log(1 + noisy_mag).unsqueeze(0).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        pred_logmag = model(noisy_logmag)\n    pred_mag = torch.exp(pred_logmag.squeeze(0).squeeze(0)) - 1\n    pred_stft = pred_mag * torch.exp(1j * noisy_phase)\n    denoised_audio = torch.istft(pred_stft, n_fft=n_fft, hop_length=hop_stft, \n                                 window=torch.hann_window(n_fft))\n    return denoised_audio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T15:23:37.305137Z","iopub.execute_input":"2025-03-22T15:23:37.305338Z","iopub.status.idle":"2025-03-22T15:23:37.310963Z","shell.execute_reply.started":"2025-03-22T15:23:37.305320Z","shell.execute_reply":"2025-03-22T15:23:37.309961Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import torchaudio\n\nnoisy_dir = '/kaggle/input/valentini-noisy/noisy_trainset_28spk_wav'\nclean_dir = '/kaggle/input/valentini-noisy/clean_trainset_28spk_wav'\n\nprint(\"Collecting noisy file paths...\")\nnoisy_files = sorted([os.path.join(noisy_dir, f) for f in os.listdir(noisy_dir) if f.endswith('.wav')])\nprint(\"Collecting clean file paths...\")\nclean_files = sorted([os.path.join(clean_dir, f) for f in os.listdir(clean_dir) if f.endswith('.wav')])\n\nsample_rate = torchaudio.info(noisy_files[0]).sample_rate\nprint(f\"Dataset sample rate: {sample_rate} Hz\")\nassert sample_rate == 48000, \"Expected 48kHz for Valentini dataset!\"\n\nnoisy_train, noisy_val, clean_train, clean_val = train_test_split(\n    noisy_files, clean_files, test_size=0.2, random_state=42\n)\n\ntrain_dataset = ValentiniDataset(noisy_train, clean_train, segment_length=48000, hop_length=24000)\nval_dataset = ValentiniDataset(noisy_val, clean_val, segment_length=48000, hop_length=24000)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n\nprint(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T15:23:37.311751Z","iopub.execute_input":"2025-03-22T15:23:37.311963Z","iopub.status.idle":"2025-03-22T15:27:54.857104Z","shell.execute_reply.started":"2025-03-22T15:23:37.311941Z","shell.execute_reply":"2025-03-22T15:27:54.856240Z"}},"outputs":[{"name":"stdout","text":"Collecting noisy file paths...\nCollecting clean file paths...\nDataset sample rate: 48000 Hz\nTraining samples: 40000, Validation samples: 10256\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nmodel = UNet().to(device)\n\ntrain_model(model, train_loader, val_loader, num_epochs=50, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T15:27:54.857861Z","iopub.execute_input":"2025-03-22T15:27:54.858105Z","execution_failed":"2025-03-22T15:29:35.837Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epochs:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"077dc057e29046db8465e22e820406fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Epoch 1/50:   0%|          | 0/2500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"037c854cbedf4b7ba320902fc3cae775"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import ipywidgets as widgets\nfrom IPython.display import Audio, display\n\nrandom_idx = random.randint(0, len(noisy_val) - 1)\nnoisy_file = noisy_val[random_idx]\nprint(f\"Selected file: {noisy_file}\")\n\ndef denoise_audio_full(model, noisy_file, n_fft=512, hop_stft=128, device=device):\n    noisy_audio, sr = torchaudio.load(noisy_file)\n    noisy_stft = torch.stft(noisy_audio.squeeze(0), n_fft=n_fft, hop_length=hop_stft, \n                            window=torch.hann_window(n_fft), return_complex=True)\n    noisy_mag = torch.abs(noisy_stft)\n    noisy_phase = torch.angle(noisy_stft)\n    noisy_logmag = torch.log(1 + noisy_mag).unsqueeze(0).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        pred_logmag = model(noisy_logmag)\n    pred_mag = torch.exp(pred_logmag.squeeze(0).squeeze(0)) - 1\n    pred_stft = pred_mag * torch.exp(1j * noisy_phase)\n    denoised_audio = torch.istft(pred_stft, n_fft=n_fft, hop_length=hop_stft, \n                                 window=torch.hann_window(n_fft), length=noisy_audio.size(1))\n    return denoised_audio, sr\n\nmodel = UNet().to(device)\nmodel.load_state_dict(torch.load('/kaggle/working/best_model.pth'))\nmodel.eval()\n\ndenoised_audio, sr = denoise_audio_full(model, noisy_file)\ntorchaudio.save('/kaggle/working/denoised_test.wav', denoised_audio.unsqueeze(0), sr)\n\nnoisy_audio, _ = torchaudio.load(noisy_file)\n\nplay_noisy_btn = Button(description=\"Play Noisy\")\nplay_denoised_btn = Button(description=\"Play Denoised\")\nvolume_slider = FloatSlider(value=1.0, min=0.0, max=1.0, step=0.1, description=\"Volume\")\n\ndef play_noisy(b):\n    display(Audio(noisy_audio.numpy(), rate=sr, normalize=False, volume=volume_slider.value))\n\ndef play_denoised(b):\n    display(Audio(denoised_audio.numpy(), rate=sr, normalize=False, volume=volume_slider.value))\n\nplay_noisy_btn.on_click(play_noisy)\nplay_denoised_btn.on_click(play_denoised)\n\nui = VBox([\n    HBox([play_noisy_btn, play_denoised_btn]),\n    volume_slider\n])\n\ndisplay(ui)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-22T15:29:35.838Z"}},"outputs":[],"execution_count":null}]}